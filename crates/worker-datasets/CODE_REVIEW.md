## Overview
This worker ingests dataset batches from S3, generates embeddings via external providers, and writes vectors to Qdrant. The flow is straightforward but several choices trade correctness and scalability for simplicity. Below are the most impactful issues to address before scaling up.

## Critical
- **Unbounded dataset ingestion blows up memory** – `process_vector_job` loads the entire S3 object into RAM via `get_file` and immediately deserializes the full JSON payload, then materializes all `PointStruct`s before the upload. On large datasets this will spike resident memory and can crash the pod or trigger OOM evictions. See [crates/worker-datasets/src/job.rs#L77-L113](crates/worker-datasets/src/job.rs#L77-L113) and the underlying helper lacking size checks at [crates/core/src/storage.rs#L202-L221](crates/core/src/storage.rs#L202-L221). A guarded variant already exists at [crates/core/src/storage.rs#L223-L239](crates/core/src/storage.rs#L223-L239); consider using that (or streaming chunks straight into processing) plus iterating batches instead of buffering the entire vector job.

## High
- **Per-job Qdrant client construction prevents connection reuse** – Every job builds a fresh `Qdrant` client and performs collection introspection synchronously. For higher concurrency this wastes TLS handshakes and ignores connection pooling. See [crates/worker-datasets/src/job.rs#L188-L224](crates/worker-datasets/src/job.rs#L188-L224). Lift client creation into the `WorkerContext`, wrap it in an `Arc`, and configure request timeouts so JetStream backpressure does not exhaust file descriptors.
- **Vector collection bootstrap ignores scalable settings** – Newly created collections accept the defaults (single shard, RAM storage, disabled write-ahead logs). At Kubernetes scale you will need to set `shard_number`, `replication_factor`, and `optimizers_config` for throughput, especially when multiple workers publish simultaneously. The current create call at [crates/worker-datasets/src/job.rs#L211-L224](crates/worker-datasets/src/job.rs#L211-L224) hardcodes only size/distance. Add configuration knobs (per job or via config service) so large tenants can tune durability and performance.
- **Upserts fire-and-forget without verifying completion** – `upsert_points` returns an operation response (with status/operation ID) but the code drops it and uses the default `wait` flag (`false`). A congested Qdrant node can enqueue the operation and later fail, while we already publish success back to JetStream. Capture the response at [crates/worker-datasets/src/job.rs#L241-L252](crates/worker-datasets/src/job.rs#L241-L252), enable `.wait(true)` for synchronous guarantees (or poll the operation), and surface per-point errors to observability.

## Medium
- **Embedding client logic duplicated and diverging across workers** – The dataset worker owns `generate_batch_embeddings` with HTTP retry/backoff, while the collections worker ships a near-copy in its semantic chunking strategy. Compare [crates/worker-datasets/src/embedder.rs#L25-L184](crates/worker-datasets/src/embedder.rs#L25-L184) with [crates/worker-collections/src/chunk/strategies/semantic.rs#L125-L291](crates/worker-collections/src/chunk/strategies/semantic.rs#L125-L291). Centralize this in a shared module (likely under `crates/core`) so provider quirks, auth headers, and pooling stay consistent.
- **Large Qdrant batches built in a single vector** – The worker zips all items with embeddings into one `Vec<PointStruct>` before sending. For very large batches this replicates data, increases peak memory, and misses the opportunity to stream smaller chunks with retries. Consider chunking uploads (e.g., 1–5k points per update) and pacing them with progress updates.

## Low
- **Status updates lack granularity** – Only a "processing" heartbeat is sent at the start, then a final result. Adding intermediate calls to `send_progress_update` (e.g., after each upload chunk) would make UI feedback smoother and let operators spot stalls early.
- **Error classification collapses all JetStream failures** – Any failure in Qdrant or embedding returns `Err((chunk_count, message))`, but the `chunk_count` is sometimes still zero even if partial work succeeded. Consider reporting the last successful index so retry logic can resume where it left off.
