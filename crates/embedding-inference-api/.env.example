# Inference API - Environment Configuration Example
# Copy this file to .env and adjust values for your environment.
# For production, use proper secrets management (e.g., Kubernetes Secrets, Vault).

#==============================================================================
# LOGGING
#==============================================================================

# Rust logging filter (see https://docs.rs/tracing-subscriber for syntax)
RUST_LOG="warn,inference_api=info"

# Log format: "json" for production, "pretty" for development
LOG_FORMAT="json"

# ============================================================================
# CUDA/GPU Configuration
# ============================================================================
# Set CUDA_VISIBLE_DEVICES to specify which GPU(s) to use
# Examples:
#   CUDA_VISIBLE_DEVICES=0          # Use first GPU
#   CUDA_VISIBLE_DEVICES=0,1        # Use first two GPUs
#   CUDA_VISIBLE_DEVICES=1          # Use second GPU
# Leave unset or empty to use all available GPUs (default)
CUDA_VISIBLE_DEVICES=0

#==============================================================================
# SERVER CONFIGURATION
#==============================================================================

# Bind address for the HTTP server
INFERENCE_HOSTNAME="0.0.0.0"

# HTTP server port
INFERENCE_PORT="8090"

# CORS allowed origins (comma-separated, or "*" for all)
CORS_ALLOWED_ORIGINS="*"

#==============================================================================
# MODEL CONFIGURATION
#==============================================================================

# HuggingFace cache directory for downloaded models
# For airgapped deployments, pre-download models and set this path
HF_HOME="/models"

# HuggingFace endpoint URL (for Artifactory proxy or local mirror)
# Set this for airgapped environments with an internal HF mirror
# HF_ENDPOINT="https://artifactory.company.com/huggingface"

# Custom model directory for user-provided ONNX models
# INFERENCE_MODEL_PATH="/path/to/custom/models"

# Comma-separated list of allowed embedding models (REQUIRED)
# Set to '*' for all supported models, or a specific comma-separated list
# If not set or empty, the server will fail to start
# Examples:
#   INFERENCE_ALLOWED_EMBEDDING_MODELS="*"
#   INFERENCE_ALLOWED_EMBEDDING_MODELS="BAAI/bge-small-en-v1.5,sentence-transformers/all-MiniLM-L6-v2"
INFERENCE_ALLOWED_EMBEDDING_MODELS="BAAI/bge-small-en-v1.5"

# Comma-separated list of allowed rerank models (optional)
# Set to '*' for all supported models, a specific list, or omit entirely for no rerankers
# Examples:
#   INFERENCE_ALLOWED_RERANK_MODELS="*"
#   INFERENCE_ALLOWED_RERANK_MODELS="BAAI/bge-reranker-base"
#   (omit or leave empty for no rerankers)
INFERENCE_ALLOWED_RERANK_MODELS="BAAI/bge-reranker-base"

# Maximum batch size for embedding requests
INFERENCE_MAX_BATCH_SIZE="256"

# Maximum concurrent embedding requests (for backpressure control)
# When at capacity, the API returns 503 with Retry-After header
# The caller should respect Retry-After and retry. Default is 2 for GPU memory safety.
INFERENCE_MAX_CONCURRENT_REQUESTS="2"

#==============================================================================
# GPU/CUDA CONFIGURATION
#==============================================================================
# IMPORTANT: CUDA is REQUIRED - this service will not run without GPU acceleration.
# The service always uses CUDA; there is no CPU fallback.

# CUDA device ID to use (optional)
# If not set, CUDA will use CUDA_VISIBLE_DEVICES environment variable or default device
# In Kubernetes/Docker Compose, use CUDA_VISIBLE_DEVICES to control GPU allocation
# INFERENCE_CUDA_DEVICE_ID="0"

#==============================================================================
# TLS/SSL CONFIGURATION (for HTTPS)
#==============================================================================

# Enable TLS/SSL for HTTPS connections
SERVER_SSL_ENABLED="false"

# Path to TLS certificate file (PEM format)
# SERVER_SSL_CERT_PATH="/etc/ssl/certs/embedding-inference-api.crt"

# Path to TLS private key file (PEM format)
# SERVER_SSL_KEY_PATH="/etc/ssl/private/embedding-inference-api.key"

#==============================================================================
# OBSERVABILITY
#==============================================================================

# OpenTelemetry OTLP endpoint for traces and metrics
OTEL_EXPORTER_OTLP_ENDPOINT="http://otel-collector:4317"

# Service name for tracing (appears in Grafana/Jaeger)
SERVICE_NAME="embedding-inference-api"
