RUST_LOG="info,actix_web_prom=error"
LOG_FORMAT="pretty"
INFERENCE_HOSTNAME="0.0.0.0"
INFERENCE_PORT="8090"
CORS_ALLOWED_ORIGINS="*"

#==============================================================================
# MODEL CONFIGURATION
#==============================================================================
# HuggingFace cache directory (models will be downloaded here)
# HF_HOME="/path/to/models"

# HuggingFace endpoint (for Artifactory proxy or local mirror in airgapped environments)
# HF_ENDPOINT="https://huggingface.co"

# Custom model directory for user-provided ONNX models
# INFERENCE_MODEL_PATH="/path/to/custom/models"

# Comma-separated list of allowed models
# If empty or not set, all models are allowed
# Only models in this list will be available via /api/models/list and accepted by /api/embed and /api/rerank
# INFERENCE_ALLOWED_MODELS="BAAI/bge-small-en-v1.5,sentence-transformers/all-MiniLM-L6-v2"

# Maximum batch size for embedding requests
INFERENCE_MAX_BATCH_SIZE="256"

SERVER_SSL_ENABLED="false"
# SERVER_SSL_CERT_PATH="/path/to/cert.pem"
# SERVER_SSL_KEY_PATH="/path/to/key.pem"

OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4317"
SERVICE_NAME="inference-api"
