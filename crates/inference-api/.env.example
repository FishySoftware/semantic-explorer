# Inference API - Environment Configuration Example
# Copy this file to .env and adjust values for your environment.
# For production, use proper secrets management (e.g., Kubernetes Secrets, Vault).

#==============================================================================
# LOGGING
#==============================================================================

# Rust logging filter (see https://docs.rs/tracing-subscriber for syntax)
RUST_LOG="warn,inference_api=info"

# Log format: "json" for production, "pretty" for development
LOG_FORMAT="json"

# ============================================================================
# CUDA/GPU Configuration
# ============================================================================
# Set CUDA_VISIBLE_DEVICES to specify which GPU(s) to use
# Examples:
#   CUDA_VISIBLE_DEVICES=0          # Use first GPU
#   CUDA_VISIBLE_DEVICES=0,1        # Use first two GPUs
#   CUDA_VISIBLE_DEVICES=1          # Use second GPU
# Leave unset or empty to use all available GPUs (default)
CUDA_VISIBLE_DEVICES=0

#==============================================================================
# SERVER CONFIGURATION
#==============================================================================

# Bind address for the HTTP server
INFERENCE_HOSTNAME="0.0.0.0"

# HTTP server port
INFERENCE_PORT="8090"

# CORS allowed origins (comma-separated, or "*" for all)
CORS_ALLOWED_ORIGINS="*"

#==============================================================================
# MODEL CONFIGURATION
#==============================================================================

# HuggingFace cache directory for downloaded models
# For airgapped deployments, pre-download models and set this path
HF_HOME="/models"

# HuggingFace endpoint URL (for Artifactory proxy or local mirror)
# Set this for airgapped environments with an internal HF mirror
# HF_ENDPOINT="https://artifactory.company.com/huggingface"

# Custom model directory for user-provided ONNX models
# INFERENCE_MODEL_PATH="/path/to/custom/models"

# Comma-separated list of models to preload at startup
# Preloading reduces first-request latency
INFERENCE_PRELOAD_MODELS="BAAI/bge-small-en-v1.5,BAAI/bge-reranker-base"

# Default embedding model (used when request doesn't specify model)
INFERENCE_DEFAULT_EMBEDDING_MODEL="BAAI/bge-small-en-v1.5"

# Default reranker model (used when request doesn't specify model)
INFERENCE_DEFAULT_RERANKER_MODEL="BAAI/bge-reranker-base"

# Maximum batch size for embedding requests
INFERENCE_MAX_BATCH_SIZE="256"

#==============================================================================
# GPU/CUDA CONFIGURATION
#==============================================================================
# IMPORTANT: CUDA is REQUIRED - this service will not run without GPU acceleration.
# The service always uses CUDA; there is no CPU fallback.

# CUDA device ID to use (optional)
# If not set, CUDA will use CUDA_VISIBLE_DEVICES environment variable or default device
# In Kubernetes/Docker Compose, use CUDA_VISIBLE_DEVICES to control GPU allocation
# INFERENCE_CUDA_DEVICE_ID="0"

#==============================================================================
# TLS/SSL CONFIGURATION (for HTTPS)
#==============================================================================

# Enable TLS/SSL for HTTPS connections
SERVER_SSL_ENABLED="false"

# Path to TLS certificate file (PEM format)
# SERVER_SSL_CERT_PATH="/etc/ssl/certs/inference-api.crt"

# Path to TLS private key file (PEM format)
# SERVER_SSL_KEY_PATH="/etc/ssl/private/inference-api.key"

#==============================================================================
# OBSERVABILITY
#==============================================================================

# OpenTelemetry OTLP endpoint for traces and metrics
OTEL_EXPORTER_OTLP_ENDPOINT="http://otel-collector:4317"

# Service name for tracing (appears in Grafana/Jaeger)
SERVICE_NAME="inference-api"
