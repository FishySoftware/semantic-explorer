# LLM Inference API - Environment Configuration Example
# Copy this file to .env and adjust values for your environment.
# For production, use proper secrets management (e.g., Kubernetes Secrets, Vault).

#==============================================================================
# LOGGING
#==============================================================================

# Rust logging filter (see https://docs.rs/tracing-subscriber for syntax)
RUST_LOG="warn,llm_inference_api=info"

# Log format: "json" for production, "pretty" for development
LOG_FORMAT="json"

#==============================================================================
# CUDA/GPU Configuration
#==============================================================================
# Set CUDA_VISIBLE_DEVICES to specify which GPU(s) to use
# Examples:
#   CUDA_VISIBLE_DEVICES=0          # Use first GPU
#   CUDA_VISIBLE_DEVICES=0,1        # Use first two GPUs
#   CUDA_VISIBLE_DEVICES=1          # Use second GPU
# Leave unset or empty to use all available GPUs (default)
CUDA_VISIBLE_DEVICES=0

#==============================================================================
# SERVER CONFIGURATION
#==============================================================================

# Bind address for the HTTP server
LLM_INFERENCE_HOSTNAME="0.0.0.0"

# HTTP server port
LLM_INFERENCE_PORT="8091"

# CORS allowed origins (comma-separated, or "*" for all)
CORS_ALLOWED_ORIGINS="*"

#==============================================================================
# MODEL CONFIGURATION
#==============================================================================

# HuggingFace cache directory for downloaded models
# For airgapped deployments, pre-download models and set this path
HF_HOME="/models"

# HuggingFace endpoint URL (for Artifactory proxy or local mirror)
# Set this for airgapped environments with an internal HF mirror
# HF_ENDPOINT="https://artifactory.company.com/huggingface"

# Custom model directory for user-provided models (GGUF, SafeTensors, etc.)
# LLM_MODEL_PATH="/path/to/custom/models"

# Comma-separated list of allowed LLM models (REQUIRED)
# GGUF models MUST use format: "repo:filename.gguf" or "repo:filename.gguf@tokenizer-repo"
#
# 1. GGUF models (CPU, CUDA, Metal)
#   Format: "repo:filename.gguf" (required - no auto-detection)
#   Optional: "repo:filename.gguf@tokenizer-repo" to override tokenizer
#   Examples:
#     - "TheBloke/Mistral-7B-Instruct-v0.2-GGUF:mistral-7b-instruct-v0.2.Q4_K_M.gguf"
#     - "microsoft/Phi-3-mini-4k-instruct-gguf:Phi-3-mini-4k-instruct-q4.gguf"
#     - "bartowski/Llama-3-8B-GGUF:Llama-3-8B-Q8_0.gguf@meta-llama/Meta-Llama-3-8B"
#   Browse files: https://huggingface.co/{repo}/tree/main
#
# 2. GPTQ models (CUDA only)
#   Format: "owner/repo"
#   Examples:
#     - "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
#     - "kaitchup/Phi-3-mini-4k-instruct-gptq-4bit"
#
# 3. Standard HuggingFace models
#   Format: "owner/repo"
#   Examples:
#     - "mistralai/Mistral-7B-Instruct-v0.2"
#     - "meta-llama/Llama-2-7b-chat-hf"
LLM_ALLOWED_MODELS="TheBloke/Mistral-7B-Instruct-v0.2-GGUF:mistral-7b-instruct-v0.2.Q4_K_M.gguf"

# Maximum number of concurrent inference requests
# When at capacity, the API returns 503 with Retry-After header (backpressure)
# Adjust based on GPU memory and model size
LLM_MAX_CONCURRENT_REQUESTS="10"

#==============================================================================
# QUANTIZATION CONFIGURATION
#==============================================================================

# Enable ISQ (In-Situ Quantization) for HuggingFace models
# This quantizes models during loading to reduce memory usage
# NOTE: GGUF models are pre-quantized and don't need ISQ
# WARNING: ISQ is SLOW on first load (5-10 minutes) and NOT cached between restarts!
# Set to "true" to enable runtime quantization, "false" to disable (default)
LLM_ENABLE_ISQ="false"

# ISQ quantization type (only used if LLM_ENABLE_ISQ=true)
# Options: Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, Q8_1, Q2_K, Q3_K, Q4_K, Q5_K, Q6_K, Q8_K
# Smaller numbers = more compression, less quality
# Recommended: Q4_K (good balance) or Q8_0 (high quality)
# LLM_ISQ_TYPE="Q4_K"
#
# Memory usage with 7B models:
# - Full precision: ~14GB VRAM
# - Q8_0: ~8GB VRAM
# - Q4_K: ~4GB VRAM
#
# TIP: Use pre-quantized GGUF models instead of ISQ:
# - Loads in seconds (vs 5-10 minutes)
# - Cached forever
# - Same quality
# Example: LLM_ALLOWED_MODELS="TheBloke/Mistral-7B-Instruct-v0.2-GGUF"

#==============================================================================
# TEXT GENERATION DEFAULTS
#==============================================================================

# Default temperature for sampling (0.0-2.0)
# Lower = more deterministic, Higher = more creative
LLM_DEFAULT_TEMPERATURE="0.7"

# Default top_p for nucleus sampling (0.0-1.0)
# Controls diversity of token selection
LLM_DEFAULT_TOP_P="0.9"

# Default maximum tokens to generate
LLM_DEFAULT_MAX_TOKENS="512"

# Hard limit on maximum tokens (safety)
# Prevents excessive token generation
LLM_MAX_TOKENS_LIMIT="4096"

#==============================================================================
# PAGED ATTENTION CONFIGURATION
#==============================================================================

# Paged attention block size (default: 32)
# Controls the granularity of memory allocation for attention computation
# Smaller values = finer granularity but more overhead
# Typical values: 16, 32, 64
LLM_PAGED_ATTENTION_BLOCK_SIZE="32"

# Paged attention GPU memory context size (default: 1024)
# Controls the maximum context size allocated in GPU memory for paged attention
# Larger values allow longer sequences but use more VRAM
# Adjust based on your model and available GPU memory
# Typical values: 512, 1024, 2048, 4096
LLM_PAGED_ATTENTION_CONTEXT_SIZE="1024"

#==============================================================================
# GPU/CUDA CONFIGURATION
#==============================================================================
# IMPORTANT: CUDA is REQUIRED - this service will not run without GPU acceleration.
# The service always uses CUDA via mistral.rs; there is no CPU fallback.

# CUDA device ID to use (optional)
# If not set, CUDA will use CUDA_VISIBLE_DEVICES environment variable or default device
# In Kubernetes/Docker Compose, use CUDA_VISIBLE_DEVICES to control GPU allocation
# LLM_CUDA_DEVICE_ID="0"

#==============================================================================
# TLS/SSL CONFIGURATION (for HTTPS)
#==============================================================================

# Enable TLS/SSL for HTTPS connections
SERVER_SSL_ENABLED="false"

# Path to TLS certificate file (PEM format)
# SERVER_SSL_CERT_PATH="/etc/ssl/certs/llm-inference-api.crt"

# Path to TLS private key file (PEM format)
# SERVER_SSL_KEY_PATH="/etc/ssl/private/llm-inference-api.key"

#==============================================================================
# OBSERVABILITY
#==============================================================================

# OpenTelemetry OTLP endpoint for traces and metrics
OTEL_EXPORTER_OTLP_ENDPOINT="http://otel-collector:4317"

# Service name for tracing (appears in Grafana/Jaeger)
SERVICE_NAME="llm-inference-api"
