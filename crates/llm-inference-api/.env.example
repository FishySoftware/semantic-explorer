# LLM Inference API - Environment Configuration Example
# Copy this file to .env and adjust values for your environment.
# For production, use proper secrets management (e.g., Kubernetes Secrets, Vault).

#==============================================================================
# LOGGING
#==============================================================================

# Rust logging filter (see https://docs.rs/tracing-subscriber for syntax)
RUST_LOG="warn,llm_inference_api=info"

# Log format: "json" for production, "pretty" for development
LOG_FORMAT="json"

#==============================================================================
# CUDA/GPU Configuration
#==============================================================================
# Set CUDA_VISIBLE_DEVICES to specify which GPU(s) to use
# Examples:
#   CUDA_VISIBLE_DEVICES=0          # Use first GPU
#   CUDA_VISIBLE_DEVICES=0,1        # Use first two GPUs
#   CUDA_VISIBLE_DEVICES=1          # Use second GPU
# Leave unset or empty to use all available GPUs (default)
CUDA_VISIBLE_DEVICES=0

#==============================================================================
# SERVER CONFIGURATION
#==============================================================================

# Bind address for the HTTP server
LLM_INFERENCE_HOSTNAME="0.0.0.0"

# HTTP server port
LLM_INFERENCE_PORT="8091"

# CORS allowed origins (comma-separated, or "*" for all)
CORS_ALLOWED_ORIGINS="*"

#==============================================================================
# MODEL CONFIGURATION
#==============================================================================

# HuggingFace cache directory for downloaded models
# For airgapped deployments, pre-download models and set this path
HF_HOME="/models"

# HuggingFace endpoint URL (for Artifactory proxy or local mirror)
# Set this for airgapped environments with an internal HF mirror
# HF_ENDPOINT="https://artifactory.company.com/huggingface"

# Custom model directory for user-provided models (GGUF, SafeTensors, etc.)
# LLM_MODEL_PATH="/path/to/custom/models"

# Comma-separated list of allowed LLM models
# If empty, all models are allowed (not recommended for production)
# Examples:
#   LLM_ALLOWED_MODELS="mistralai/Mistral-7B-Instruct-v0.2,meta-llama/Llama-2-7b-chat-hf"
LLM_ALLOWED_MODELS="mistralai/Mistral-7B-Instruct-v0.2"

# Default model to use when not specified in request
LLM_DEFAULT_MODEL="mistralai/Mistral-7B-Instruct-v0.2"

# Maximum number of concurrent inference requests
# Adjust based on GPU memory and model size
LLM_MAX_CONCURRENT_REQUESTS="10"

#==============================================================================
# TEXT GENERATION DEFAULTS
#==============================================================================

# Default temperature for sampling (0.0-2.0)
# Lower = more deterministic, Higher = more creative
LLM_DEFAULT_TEMPERATURE="0.7"

# Default top_p for nucleus sampling (0.0-1.0)
# Controls diversity of token selection
LLM_DEFAULT_TOP_P="0.9"

# Default maximum tokens to generate
LLM_DEFAULT_MAX_TOKENS="512"

# Hard limit on maximum tokens (safety)
# Prevents excessive token generation
LLM_MAX_TOKENS_LIMIT="4096"

#==============================================================================
# GPU/CUDA CONFIGURATION
#==============================================================================
# IMPORTANT: CUDA is REQUIRED - this service will not run without GPU acceleration.
# The service always uses CUDA via mistral.rs; there is no CPU fallback.

# CUDA device ID to use (optional)
# If not set, CUDA will use CUDA_VISIBLE_DEVICES environment variable or default device
# In Kubernetes/Docker Compose, use CUDA_VISIBLE_DEVICES to control GPU allocation
# LLM_CUDA_DEVICE_ID="0"

#==============================================================================
# TLS/SSL CONFIGURATION (for HTTPS)
#==============================================================================

# Enable TLS/SSL for HTTPS connections
SERVER_SSL_ENABLED="false"

# Path to TLS certificate file (PEM format)
# SERVER_SSL_CERT_PATH="/etc/ssl/certs/llm-inference-api.crt"

# Path to TLS private key file (PEM format)
# SERVER_SSL_KEY_PATH="/etc/ssl/private/llm-inference-api.key"

#==============================================================================
# OBSERVABILITY
#==============================================================================

# OpenTelemetry OTLP endpoint for traces and metrics
OTEL_EXPORTER_OTLP_ENDPOINT="http://otel-collector:4317"

# Service name for tracing (appears in Grafana/Jaeger)
SERVICE_NAME="llm-inference-api"
