RUST_LOG="debug,actix_web_prom=error"
LOG_FORMAT="pretty"
LLM_INFERENCE_HOSTNAME="0.0.0.0"
LLM_INFERENCE_PORT="8091"
CORS_ALLOWED_ORIGINS="*"

#==============================================================================
# MODEL CONFIGURATION
#==============================================================================
# HuggingFace cache directory (models will be downloaded here)
# Set this to a path with sufficient disk space for large language models
# HF_HOME="/path/to/models"

# HuggingFace endpoint (for Artifactory proxy or local mirror in airgapped environments)
# HF_ENDPOINT="https://huggingface.co"

# Custom model directory for user-provided models
# LLM_MODEL_PATH="/path/to/custom/models"

LLM_ALLOWED_MODELS=HuggingFaceTB/SmolLM3-3B
#"microsoft/Phi-3-mini-4k-instruct-gguf:Phi-3-mini-4k-instruct-q4.gguf@microsoft/Phi-3-mini-4k-instruct" 


# Maximum number of concurrent inference requests
# When at capacity, the API returns 503 with Retry-After header (backpressure)
# Adjust based on available VRAM/RAM
LLM_MAX_CONCURRENT_REQUESTS="3"

# Paged attention block size (default: 32)
# Controls the granularity of memory allocation for attention computation
# Smaller values = finer granularity but more overhead
# Typical values: 16, 32, 64
LLM_PAGED_ATTENTION_BLOCK_SIZE="32"

# Paged attention GPU memory context size (default: 1024)
# Controls the maximum context size allocated in GPU memory for paged attention
# Larger values allow longer sequences but use more VRAM
# Adjust based on your model and available GPU memory
# Typical values: 512, 1024, 2048, 4096
LLM_PAGED_ATTENTION_CONTEXT_SIZE="1024"

# Default temperature for generation (0.0 - 2.0)
# Lower = more deterministic, Higher = more creative
LLM_DEFAULT_TEMPERATURE="0.5"
LLM_DEFAULT_TOP_P="0.9"
LLM_DEFAULT_MAX_TOKENS="512"
LLM_MAX_TOKENS_LIMIT="4096"

SERVER_SSL_ENABLED="false"
# SERVER_SSL_CERT_PATH="/path/to/cert.pem"
# SERVER_SSL_KEY_PATH="/path/to/key.pem"

SERVICE_NAME="llm-inference-api"
OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:4317"
