# Default values for semantic-explorer
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Global configuration for all components
global:
  imageRegistry: "docker.io"
  imagePullSecrets: []
  storageClass: ""
  # Init container image for startup ordering (secure, non-root)
  initContainer:
    image: "busybox:1.36"
  securityContext:
    runAsNonRoot: true
    runAsUser: 10001
    runAsGroup: 10001
    fsGroup: 10001
    fsGroupChangePolicy: "OnRootMismatch"
    seccompProfile:
      type: RuntimeDefault
  podSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
  environment: production
  networkPolicy:
    enabled: true
  serviceMesh:
    enabled: false
    istio:
      enabled: false

networkPolicy:
  enabled: true
  ingress:
    # Allow ingress from ingress controller
    fromIngress: true
    # Allow ingress from specific namespaces
    fromNamespaces: []
    #  - name: ingress-nginx
    # Allow ingress from specific pods
    fromPods: []
  egress:
    # Allow egress to DNS
    toDns: true
    # Allow egress to internet
    toInternet: true
    # Allow egress to specific namespaces
    toNamespaces: []
    # Allow egress to specific CIDR blocks
    toCidr: []
    #  - 10.0.0.0/8
commonLabels: {}
commonAnnotations: {}


# API Service Configuration
api:
  enabled: true
  replicaCount: 3
  image:
    repository: jofish89/semantic-explorer
    tag: "latest"
    pullPolicy: Always
  statefulset:
    podManagementPolicy: Parallel
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0
  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080
    annotations: {}
  ingress:
    enabled: true
    className: "nginx"
    annotations: {}
    hosts:
      - host: api.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: []
    #  - secretName: api-tls
    #    hosts:
    #      - api.example.com
  resources:
    requests:
      cpu: 1000m
      memory: 1Gi
    limits:
      cpu: 4000m
      memory: 8Gi
  autoscaling:
    enabled: true
    minReplicas: 3
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  livenessProbe:
    httpGet:
      path: /health/live
      port: 8080
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ready
      port: 8080
    initialDelaySeconds: 10
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
    successThreshold: 1
  startupProbe:
    httpGet:
      path: /health/live
      port: 8080
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 30
    successThreshold: 1
  env:
    RUST_LOG: "warn,actix_web_prom=error,semantic_explorer=info"
    HOSTNAME: "0.0.0.0"
    SHUTDOWN_TIMEOUT_SECS: "30"
    LOG_FORMAT: "json"
    # PUBLIC_URL is used for external-facing URLs like OIDC callbacks
    # Set this to your ingress URL (e.g., https://semantic-explorer.example.com)
    # PUBLIC_URL: ""
    DB_MIN_CONNECTIONS: "2"
    DB_MAX_CONNECTIONS: "20"
    DB_ACQUIRE_TIMEOUT_SECS: "30"
    DB_IDLE_TIMEOUT_SECS: "300"
    DB_MAX_LIFETIME_SECS: "1800"
    QDRANT_TIMEOUT_SECS: "30"
    QDRANT_CONNECT_TIMEOUT_SECS: "10"
    NATS_REPLICAS: "3"
    # Reconciliation job for transform reliability (retries failed batch publishes)
    RECONCILIATION_INTERVAL_SECS: "300"
    # Worker batch sizes
    WORKER_SEARCH_BATCH_SIZE: "200"
    WORKER_CHAT_BATCH_SIZE: "500"
    WORKER_DATASET_BATCH_SIZE: "1000"
    WORKER_S3_DELETE_BATCH_SIZE: "1000"
    WORKER_QDRANT_UPLOAD_CHUNK_SIZE: "200"
    # Scanner configuration
    DATASET_SCANNER_MAX_PENDING: "500"
    DATASET_SCANNER_BATCH_DELAY_MS: "0"
    DATASET_SCANNER_TIMEOUT_SECS: "300"
    DATASET_SCANNER_MAX_BATCHES_PER_SCAN: "1000"
    DATASET_SCANNER_MAX_ITEMS_PER_SCAN: "100000"
    STUCK_BATCH_THRESHOLD_HOURS: "2"
  envFrom: []
  nodeSelector: {}
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - api
            topologyKey: kubernetes.io/hostname
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  priorityClassName: ""
  serviceAccount:
    create: true
    annotations: {}
    name: "sa-semantic-explorer-api"
  podAnnotations: {}
  podLabels: {}
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 15s
      scrapeTimeout: 5s

# Worker Collections Configuration
workerCollections:
  enabled: true
  replicaCount: 3
  image:
    repository: jofish89/worker-collections
    tag: "latest"
    pullPolicy: Always
  statefulset:
    podManagementPolicy: Parallel
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0
  resources:
    requests:
      cpu: 2000m
      memory: 1Gi
    limits:
      cpu: 4000m
      memory: 2Gi
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  livenessProbe: {}
  readinessProbe: {}
  env:
    RUST_LOG: "warn,actix_web_prom=error,worker_collections=info"
    LOG_FORMAT: "json"
    NATS_REPLICAS: "3"
    MAX_CONCURRENT_JOBS: "10"
    # Worker configuration
    WORKER_QDRANT_UPLOAD_CHUNK_SIZE: "200"
    HEALTH_CHECK_PORT: "8080"
    EMBEDDING_MAX_CONCURRENT_REQUESTS: "5"
  envFrom: []
  nodeSelector: {}
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - worker-collections
            topologyKey: kubernetes.io/hostname
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  priorityClassName: ""
  serviceAccount:
    create: true
    annotations: {}
    name: "sa-semantic-explorer-worker-collections"
  podAnnotations: {}
  podLabels: {}
  metrics:
    enabled: true
    serviceMonitor:
      enabled: false
      interval: 15s
      scrapeTimeout: 5s

# Worker Datasets Configuration
workerDatasets:
  enabled: true
  replicaCount: 3
  image:
    repository: jofish89/worker-datasets
    tag: "latest"
    pullPolicy: Always
  statefulset:
    podManagementPolicy: Parallel
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0
  resources:
    requests:
      cpu: 2000m
      memory: 2Gi
    limits:
      cpu: 4000m
      memory: 8Gi
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 8
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  livenessProbe: {}
  readinessProbe: {}
  env:
    RUST_LOG: "warn,actix_web_prom=error,worker_datasets=info"
    LOG_FORMAT: "json"
    NATS_REPLICAS: "3"
    MAX_CONCURRENT_JOBS: "10"
    # Worker configuration
    WORKER_DATASET_BATCH_SIZE: "1000"
    WORKER_QDRANT_UPLOAD_CHUNK_SIZE: "200"
    HEALTH_CHECK_PORT: "8080"
    EMBEDDING_MAX_CONCURRENT_REQUESTS: "5"
    QDRANT_PARALLEL_UPLOADS: "4"
  envFrom: []
  nodeSelector: {}
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - worker-datasets
            topologyKey: kubernetes.io/hostname
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  priorityClassName: ""
  serviceAccount:
    create: true
    annotations: {}
    name: "sa-semantic-explorer-worker-datasets"
  podAnnotations: {}
  podLabels: {}
  metrics:
    enabled: true
    serviceMonitor:
      enabled: false
      interval: 15s
      scrapeTimeout: 5s

# Worker Visualizations Configuration (Python)
workerVisualizationsPy:
  enabled: true
  replicaCount: 2
  image:
    repository: jofish89/worker-visualizations-py
    tag: "latest"
    pullPolicy: Always
  # Timeout for LLM API calls (topic naming)
  llmTimeoutSeconds: 120
  statefulset:
    podManagementPolicy: Parallel
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0
  resources:
    requests:
      cpu: 2000m
      memory: 4Gi
    limits:
      cpu: 4000m
      memory: 8Gi
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 6
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  livenessProbe:
    httpGet:
      path: /health/live
      port: 8081
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ready
      port: 8081
    initialDelaySeconds: 15
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 2
    successThreshold: 1
  terminationGracePeriodSeconds: 300  # 5 minutes for in-flight jobs
  env:
    LOG_LEVEL: "INFO"
    LOG_FORMAT: "json"
    PROCESSING_TIMEOUT_SECS: "3600"
    MAX_CONCURRENT_JOBS: "3"
    MAX_ACK_PENDING: "10"
    HEALTH_CHECK_PORT: "8081"
    NATS_REPLICAS: "3"
    # Retry settings for stream subscription during startup race conditions
    NATS_STREAM_RETRY_ATTEMPTS: "30"
    NATS_STREAM_RETRY_DELAY: "2.0"
    # Disable numba caching to avoid file locator issues in containers
    NUMBA_DISABLE_CACHING: "1"
    NUMBA_CACHE_DIR: "/tmp/numba_cache"
    # Additional numba settings to prevent caching issues
    NUMBA_DEBUGINFO: "0"
    # Pull subscription settings for horizontal scaling
    NATS_BATCH_SIZE: "1"
    NATS_FETCH_TIMEOUT: "5.0"
  envFrom: []
  nodeSelector: {}
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - worker-visualizations-py
            topologyKey: kubernetes.io/hostname
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  priorityClassName: ""
  serviceAccount:
    create: true
    annotations: {}
    name: "sa-semantic-explorer-worker-visualizations-py"
  podAnnotations: {}
  podLabels: {}
  metrics:
    enabled: false

# Embedding Inference API Configuration (GPU-accelerated embeddings and reranking)
embeddingInferenceApi:
  enabled: true
  replicaCount: 2
  image:
    repository: jofish89/embedding-inference-api
    tag: "latest"
    pullPolicy: Always
  # Security context for running as non-root with GPU access
  # Note: Container image must be built to run as non-root user
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  statefulset:
    podManagementPolicy: Parallel
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0
  service:
    type: ClusterIP
    port: 8090
    targetPort: 8090
    annotations: {}
  resources:
    requests:
      cpu: 2000m
      memory: 8Gi
      nvidia.com/gpu: 1
    limits:
      cpu: 4000m
      memory: 16Gi
      nvidia.com/gpu: 1
  autoscaling:
    enabled: false  # GPU nodes typically don't benefit from autoscaling
    minReplicas: 2
    maxReplicas: 4
    targetCPUUtilizationPercentage: 70
  livenessProbe:
    httpGet:
      path: /health/live
      port: 8090
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ready
      port: 8090
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
    successThreshold: 1
  startupProbe:
    httpGet:
      path: /health/live
      port: 8090
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30  # Allow 5 minutes for model loading
    successThreshold: 1
  env:
    RUST_LOG: "warn,inference_api=info"
    LOG_FORMAT: "json"
    INFERENCE_HOSTNAME: "0.0.0.0"
    INFERENCE_PORT: "8090"
    # INFERENCE_CUDA_DEVICE_ID is not set - use CUDA_VISIBLE_DEVICES from k8s
    INFERENCE_CUDA_MEMORY_LIMIT: "0"  # 0 = no limit
    INFERENCE_CUDA_GRAPH: "false"
    # Model configuration - use '*' to allow all models or comma-separated list
    INFERENCE_ALLOWED_EMBEDDING_MODELS: "*"
    INFERENCE_ALLOWED_RERANKER_MODELS: "*"
    INFERENCE_MAX_BATCH_SIZE: "256"
    # Concurrency and model restrictions (security)
    INFERENCE_MAX_CONCURRENT_REQUESTS: "100"
    # GPU pressure threshold (0-100) - requests rejected above this
    GPU_PRESSURE_THRESHOLD: "95.0"
    # CUDA memory arena size limit (e.g. '4G', '512M', '8589934592')
    # Unset or "0" = use all available GPU memory (default)
    # CUDA_ARENA_SIZE: "4G"
    # Arena extend strategy: 'next_power_of_two' (default, fewer larger allocs)
    # or 'same_as_requested' (exact-sized allocs, more predictable)
    # CUDA_ARENA_EXTEND_STRATEGY: "next_power_of_two"
    # Comma-separated list of allowed embedding models (empty = allow all)
    # INFERENCE_ALLOWED_EMBEDDING_MODELS: "BAAI/bge-small-en-v1.5,BAAI/bge-base-en-v1.5"
    # Comma-separated list of allowed reranker models (empty = allow all)
    # INFERENCE_ALLOWED_RERANK_MODELS: "BAAI/bge-reranker-base"
    # CORS configuration for direct browser access (usually not needed)
    # CORS_ALLOWED_ORIGINS: "https://your-domain.com"
    # HuggingFace cache directory - mounted from PVC
    HF_HOME: "/models"
    # For airgapped deployments, set HF_ENDPOINT to your Artifactory proxy
    # HF_ENDPOINT: "https://artifactory.company.com/huggingface"
  envFrom: []
  # GPU node selector - adjust labels based on your cluster
  # Example: nvidia.com/gpu.present: "true" or accelerator: "nvidia"
  nodeSelector: {}
  # Tolerations for GPU nodes if they're tainted
  # Example: key: gpu, operator: Equal, value: "true", effect: NoSchedule
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - embedding-inference-api
            topologyKey: kubernetes.io/hostname
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  priorityClassName: ""
  serviceAccount:
    create: true
    annotations: {}
    name: "sa-semantic-explorer-embedding-inference-api"
  podAnnotations: {}
  podLabels: {}
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 15s
      scrapeTimeout: 5s
  # Persistent volume for model cache
  persistence:
    enabled: true
    storageClass: ""  # Use default storage class or specify
    accessMode: ReadWriteOnce
    size: 50Gi  # Adjust based on models you plan to cache
    mountPath: /models
    # For ReadWriteMany, models can be shared across pods
    # For ReadWriteOnce, each pod gets its own cache
  # Optional: TensorRT cache for optimization
  tensorrtCache:
    enabled: false
    size: 10Gi
    mountPath: /tensorrt-cache

# LLM Inference API Configuration (GPU-accelerated text generation)
llmInferenceApi:
  enabled: true
  replicaCount: 2
  image:
    repository: jofish89/llm-inference-api
    tag: "latest"
    pullPolicy: Always
  # Security context for running as non-root with GPU access
  # Note: Container image must be built to run as non-root user
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
  statefulset:
    podManagementPolicy: Parallel
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        partition: 0
  service:
    type: ClusterIP
    port: 8091
    targetPort: 8091
    annotations: {}
  resources:
    requests:
      cpu: 2000m
      memory: 8Gi
      nvidia.com/gpu: 1
    limits:
      cpu: 4000m
      memory: 16Gi
      nvidia.com/gpu: 1
  autoscaling:
    enabled: false  # GPU nodes typically don't benefit from autoscaling
    minReplicas: 2
    maxReplicas: 4
    targetCPUUtilizationPercentage: 70
  livenessProbe:
    httpGet:
      path: /health/live
      port: 8091
    initialDelaySeconds: 60
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    httpGet:
      path: /health/ready
      port: 8091
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
    successThreshold: 1
  startupProbe:
    httpGet:
      path: /health/live
      port: 8091
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30  # Allow 5 minutes for model loading
    successThreshold: 1
  env:
    RUST_LOG: "warn,llm_inference_api=info"
    LOG_FORMAT: "json"
    LLM_INFERENCE_HOSTNAME: "0.0.0.0"
    LLM_INFERENCE_PORT: "8091"
    CORS_ALLOWED_ORIGINS: "*"
    # Model configuration
    LLM_ALLOWED_MODELS: "microsoft/Phi-3-mini-4k-instruct-gguf:Phi-3-mini-4k-instruct-q4.gguf@microsoft/Phi-3-mini-4k-instruct"
    LLM_MAX_CONCURRENT_REQUESTS: "10"
    # GPU pressure threshold (0-100) - requests rejected above this
    GPU_PRESSURE_THRESHOLD: "95.0"
    LLM_DEFAULT_TEMPERATURE: "0.5"
    LLM_DEFAULT_TOP_P: "0.9"
    LLM_DEFAULT_MAX_TOKENS: "512"
    LLM_MAX_TOKENS_LIMIT: "4096"
    # Paged attention configuration
    # FP8 KV cache for H100/H200 GPUs - reduces memory ~50% and improves performance
    LLM_PAGED_CACHE_TYPE: "f8e4m3"
    # Prefix caching for multi-turn chat and RAG acceleration
    LLM_ENABLE_PREFIX_CACHING: "true"
    # HuggingFace cache directory - mounted from PVC
    HF_HOME: "/models"
    # Home directory for appuser (used by mistral.rs for ~/.cache/huggingface)
    HOME: "/home/appuser"
    # For airgapped deployments, set HF_ENDPOINT to your Artifactory proxy
    # HF_ENDPOINT: "https://artifactory.company.com/huggingface"
    SERVICE_NAME: "llm-inference-api"
  envFrom: []
  # GPU node selector - adjust labels based on your cluster
  # Example: nvidia.com/gpu.present: "true" or accelerator: "nvidia"
  nodeSelector: {}
  # Tolerations for GPU nodes if they're tainted
  # Example: key: gpu, operator: Equal, value: "true", effect: NoSchedule
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - llm-inference-api
            topologyKey: kubernetes.io/hostname
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  priorityClassName: ""
  serviceAccount:
    create: true
    annotations: {}
    name: "sa-semantic-explorer-llm-inference-api"
  podAnnotations: {}
  podLabels: {}
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      interval: 15s
      scrapeTimeout: 5s
  # Persistent volume for model cache
  persistence:
    enabled: true
    storageClass: ""  # Use default storage class or specify
    accessMode: ReadWriteOnce
    size: 100Gi  # LLM models are larger than embedding models
    mountPath: /models
    # For ReadWriteMany, models can be shared across pods
    # For ReadWriteOnce, each pod gets its own cache

# PostgreSQL Configuration (Optional Infrastructure)
# Disabled by default - configure external.* for production use
# When enabled: true, deploys PostgreSQL subchart (CloudNativePG)
postgresql:
  enabled: false
  # Use external PostgreSQL instance
  external:
    enabled: false
    host: ""
    port: 5432
    database: explorer
    username: user
    password: ""
    existingSecret: ""
    passwordKey: "password"
    # SSL mode: disable, require, verify-ca, verify-full
    sslMode: require
  # Internal PostgreSQL deployment
  image:
    registry: ""  # Defaults to global.imageRegistry
    repository: postgres
    tag: "16.3-alpine"
    pullPolicy: IfNotPresent
  # Internal PostgreSQL configuration
  auth:
    database: explorer
    username: user
    password: ""  # Generate random if empty
    existingSecret: ""
    secretKeys:
      userPasswordKey: "database-password"
  primary:
    resources:
      requests:
        cpu: 2000m
        memory: 4Gi
      limits:
        cpu: 4000m
        memory: 8Gi
    persistence:
      enabled: true
      storageClass: ""  # Defaults to global.storageClass
      accessMode: ReadWriteOnce
      size: 100Gi
    podSecurityContext:
      runAsNonRoot: true
      runAsUser: 999
      fsGroup: 999
      fsGroupChangePolicy: "OnRootMismatch"
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: false  # PostgreSQL needs writable filesystem
  backup:
    enabled: false
    schedule: "0 2 * * *"
    retention: 7
    storageClass: ""  # Defaults to global.storageClass
  metrics:
    # Enable PostgreSQL metrics exporter for Prometheus/Grafana monitoring
    # Recommended for production to track query performance, connections, etc.
    enabled: false  # Set to true for production observability
    image:
      registry: ""  # Defaults to global.imageRegistry
      repository: prometheuscommunity/postgres-exporter
      tag: latest
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 256Mi
    serviceMonitor:
      enabled: false  # Enable if using Prometheus Operator
      interval: 30s
  podTemplate:
    merge:
      spec:
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          fsGroup: 1000
          fsGroupChangePolicy: "OnRootMismatch"

# NATS Configuration (Optional Infrastructure)
# Disabled by default - configure external.* for production use
# When enabled: true, deploys NATS subchart
nats:
  enabled: false
  # Use external NATS instance
  external:
    enabled: false
    url: ""  # e.g., "nats://nats.example.com:4222"
    existingSecret: ""
    credentialsKey: "credentials"
  # Internal NATS deployment (subchart configuration)
  config:
    cluster:
      enabled: true
      replicas: 3
    jetstream:
      enabled: true
      fileStore:
        enabled: true
        dir: /data
        pvc:
          enabled: true
          size: 10Gi
          storageClassName: ""  # Defaults to global.storageClass
        maxSize: 10Gi  # Match PVC size
      memoryStore:
        enabled: true
        maxSize: 2Gi
  container:
    image:
      registry: ""  # Defaults to global.imageRegistry
      repository: nats
      tag: 2.10-alpine
      pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 100m
      memory: 2Gi
    limits:
      cpu: 2000m
      memory: 4Gi

# Qdrant Configuration (Optional Infrastructure)
# Disabled by default - configure external.* for production use
# When enabled: true, deploys Qdrant subchart
qdrant:
  enabled: false
  # Use external Qdrant instance
  external:
    enabled: false
    url: ""  # e.g., "http://qdrant.example.com:6334"
    apiKey: ""
    existingSecret: ""
    apiKeyKey: "api-key"
  # Internal Qdrant deployment
  image:
    repository: qdrant/qdrant
    # CPU-only: use versioned tag like "v1.16.3"
    # GPU-accelerated: use "gpu-nvidia-latest" (CUDA 12+, requires nvidia GPU node)
    # GPU provides 3-10x faster indexing for large collections (>1M vectors)
    tag: "v1.16.3"
    pullPolicy: IfNotPresent
  # Replica count: 3+ recommended for production (enables raft consensus)
  # 2 replicas provide basic HA but cannot recover from single node loss
  replicaCount: 3
  # GPU support for accelerated indexing
  # To enable GPU mode:
  # 1. Set image.tag to "gpu-nvidia-latest"
  # 2. Set gpu.enabled to true
  # 3. Set gpu.nvidia.enabled to true
  # 4. Uncomment nvidia.com/gpu in resources
  # 5. Add nodeSelector for GPU nodes: nvidia.com/gpu: "true"
  gpu:
    enabled: false
    nvidia:
      enabled: false
      count: 1
    # Environment variable for GPU indexing (QDRANT__GPU__INDEXING=1)
    indexing: true
  
  # =============================================================================
  # PERFORMANCE TUNING - Qdrant
  # =============================================================================
  # These settings help prevent connection refusals and improve throughput
  # See: https://qdrant.tech/documentation/guides/configuration/
  
  # Qdrant server configuration (passed to subchart's config: key)
  # These map directly to Qdrant's YAML configuration
  config:
    cluster:
      enabled: true
      p2p:
        port: 6335
      consensus:
        # Lower tick period for faster failure detection (default: 100)
        tick_period_ms: 100
    
    service:
      # Maximum POST request size in MB (default: 32)
      max_request_size_mb: 64
      # Number of parallel workers for serving API (0 = auto based on CPU cores)
      max_workers: 0
      # gRPC port - critical for high-throughput operations
      grpc_port: 6334
      # Enable CORS for web clients
      enable_cors: true
    
    storage:
      # Performance settings
      performance:
        # Number of parallel threads for search operations (0 = auto)
        max_search_threads: 0
        # CPU budget for optimization jobs (0 = auto, negative = leave N CPUs free)
        optimizer_cpu_budget: 0
        # Rate limit for concurrent updates in distributed mode (null = auto)
        # Prevents overload during bulk operations - set to lower value if seeing connection issues
        update_rate_limit: null
        # Limit incoming automatic shard transfers (prevents resource exhaustion)
        incoming_shard_transfers_limit: 1
        # Limit outgoing automatic shard transfers
        outgoing_shard_transfers_limit: 1
      
      # HNSW index settings (affects search speed vs memory tradeoff)
      hnsw_index:
        # Edges per node in index graph (higher = more accurate, more memory)
        m: 16
        # Neighbours considered during index building (higher = better quality, slower build)
        ef_construct: 100
        # Maximum threads for background index building (0 = auto, 8-16 recommended)
        max_indexing_threads: 0
        # Store HNSW index on disk (saves RAM, slightly slower searches)
        on_disk: false
      
      # Optimizer settings (affects write performance and segment management)
      optimizers:
        # Target number of segments (0 = auto based on CPU count)
        default_segment_number: 0
        # Flush interval in seconds (lower = more durable, higher write load)
        flush_interval_sec: 5
        # Threshold for triggering vector indexing (in KB, 1KB â‰ˆ 1 vector of size 256)
        indexing_threshold_kb: 20000
        # Maximum optimization threads per shard (null = auto)
        max_optimization_threads: null
      
      # Collection defaults for new collections
      collection:
        # Default replication factor (2+ for HA)
        replication_factor: 2
        # Number of replicas that must acknowledge writes
        write_consistency_factor: 1
  
  # Environment variables for Qdrant container (alternative to config file)
  # Use QDRANT__ prefix with double underscores for nested config
  env: {}
  # Example:
  #   QDRANT__LOG_LEVEL: "INFO"
  #   QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS: "0"
  
  # Resource allocation - INCREASED for production workloads
  # Qdrant is memory-intensive; allocate generously for large collections
  resources:
    requests:
      cpu: 6000m
      memory: 8Gi
      # Uncomment for GPU mode:
      # nvidia.com/gpu: 1
    limits:
      cpu: 8000m
      memory: 8Gi
      # Uncomment for GPU mode:
      # nvidia.com/gpu: 1
  
  persistence:
    enabled: true
    storageClass: ""  # Defaults to global.storageClass
    accessMode: ReadWriteOnce
    size: 100Gi
  service:
    type: ClusterIP
    http:
      port: 6333
    grpc:
      port: 6334
  
  # Horizontal Pod Autoscaling (requires metrics-server)
  autoscaling:
    enabled: false
    minReplicas: 3
    maxReplicas: 6
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
  
  # Pod Disruption Budget for HA
  podDisruptionBudget:
    enabled: true
    # At least 2 pods must be available (for 3-replica cluster)
    minAvailable: 2
  
  # Anti-affinity to spread pods across nodes
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: qdrant
            topologyKey: kubernetes.io/hostname
  
  updateVolumeFsOwnership: false
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000
    fsGroup: 1000
    fsGroupChangePolicy: "OnRootMismatch"
    seccompProfile:
      type: RuntimeDefault
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: false  # Qdrant needs writable filesystem
  metrics:
    enabled: true
    serviceMonitor:
      enabled: false
      interval: 30s
  nodeSelector: {}
  tolerations: []

# Dex (OIDC Provider) Configuration (Optional Infrastructure)
dex:
  enabled: true
  # Use external OIDC provider
  external:
    enabled: false
    issuerUrl: ""  # e.g., "https://accounts.google.com"
    clientId: ""
    clientSecret: ""
    existingSecret: ""
    clientIdKey: "client-id"
    clientSecretKey: "client-secret"
  # Internal Dex deployment
  image:
    registry: ""  # Defaults to global.imageRegistry
    repository: dexidp/dex
    tag: "latest"
    pullPolicy: IfNotPresent
  config:
    issuer: ""  # e.g., "https://dex.example.com"
    # Storage backend - use sqlite3 for namespace-only deployments
    # Note: kubernetes storage backend requires CRDs (cluster-wide resources)
    # For production with multi-replica HA, use external DB (postgres/mysql/etcd)
    storage:
      type: sqlite3
      config:
        file: /data/dex.db
    web:
      http: 0.0.0.0:5556
    oauth2:
      skipApprovalScreen: true
    staticClients:
      - id: semantic-explorer
        redirectURIs:
          - "http://localhost:8080/auth/callback"
        name: "Semantic Explorer"
        secret: ""  # Generate random if empty
    connectors: []
    # Example GitHub connector
    #  - type: github
    #    id: github
    #    name: GitHub
    #    config:
    #      clientID: $GITHUB_CLIENT_ID
    #      clientSecret: $GITHUB_CLIENT_SECRET
    #      redirectURI: https://dex.example.com/callback
  resources:
    requests:
      cpu: 500m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 512Mi
  service:
    type: ClusterIP
    port: 5556
  ingress:
    enabled: false
    className: "nginx"
    annotations: {}
    hosts:
      - host: dex.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: []
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1001
    fsGroup: 1001
    fsGroupChangePolicy: "OnRootMismatch"
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: true
  env: {}
  envFrom: []
  nodeSelector: {}
  tolerations: []
  affinity: {}
  serviceAccount:
    create: true
    annotations: {}
    name: ""

# MinIO (S3-compatible storage) Configuration (Optional Infrastructure)
# Disabled by default - use S3-compatible storage or enable for development
# When enabled: true, deploys MinIO subchart
minio:
  enabled: false
  # Use external S3-compatible storage
  external:
    enabled: false
    endpoint: ""  # e.g., "https://s3.amazonaws.com" or "http://minio.example.com:9000"
    region: "us-east-1"
    bucket: ""  # Default bucket name
    accessKeyId: ""
    secretAccessKey: ""
    existingSecret: ""
    accessKeyIdKey: "access-key-id"
    secretAccessKeyKey: "secret-access-key"
  # MinIO deployment
  image:
    registry: ""  # Defaults to global.imageRegistry
    repository: minio/minio
    tag: "latest"
    pullPolicy: IfNotPresent
  # MinIO mode: standalone or distributed
  # distributed mode requires 4+ replicas for erasure coding
  mode: distributed
  replicas: 4
  # Drives per server (for distributed mode with multiple drives per node)
  drivesPerNode: 1
  rootUser: admin
  rootPassword: ""  # Generate random if empty
  existingSecret: ""
  
  # =============================================================================
  # PERFORMANCE TUNING - MinIO
  # =============================================================================
  # These settings optimize throughput and reduce latency for object storage
  # See: https://min.io/docs/minio/linux/reference/minio-server/settings/
  
  # Environment variables for MinIO (passed to subchart's environment: key)
  # These are the actual MinIO server environment variables
  environment:
    # API Performance Settings
    # Maximum number of concurrent requests per server (0 = unlimited)
    # Increase if seeing "connection refused" or "too many requests" errors  
    MINIO_API_REQUESTS_MAX: "0"
    # Deadline for completing requests (0s = disabled)
    MINIO_API_REQUESTS_DEADLINE: "0s"
    # Enable cluster readiness check (helps load balancers)
    MINIO_API_CLUSTER_DEADLINE: "10s"
    # Remote transport deadline for distributed operations
    MINIO_API_REMOTE_TRANSPORT_DEADLINE: "2h"
    # Transition workers for ILM tier moves
    MINIO_API_TRANSITION_WORKERS: "100"
    # Replication workers for bucket replication
    MINIO_API_REPLICATION_WORKERS: "100"
    # Maximum replication workers
    MINIO_API_REPLICATION_MAX_WORKERS: "500"
    
    # Scanner Settings (affects background healing and lifecycle)
    # Scanner speed: default, slow, fast, or fastest
    # "slow" reduces CPU usage, "fastest" prioritizes quick scans
    MINIO_SCANNER_SPEED: "default"
    
    # Healing Settings for data integrity
    # Maximum sleep between heal operations
    MINIO_HEAL_MAX_SLEEP: "1s"
    # Maximum delay between heal operations
    MINIO_HEAL_MAX_DELAY: "0"
    
    # Note: MINIO_PROMETHEUS_AUTH_TYPE is set automatically by metrics.serviceMonitor.public
    
    # Drive synchronization - helps with NVMe/SSD performance
    # MINIO_DRIVE_SYNC: "on"
  
  # Resource allocation - INCREASED for production workloads
  # MinIO benefits from high memory for caching and CPU for erasure coding
  resources:
    requests:
      cpu: 6000m
      memory: 8Gi
    limits:
      cpu: 6000m
      memory: 8Gi
  persistence:
    enabled: true
    storageClass: ""  # Defaults to global.storageClass
    accessMode: ReadWriteOnce
    size: 100Gi
  service:
    type: ClusterIP
    port: 9000
    consolePort: 9001
  ingress:
    enabled: false
    className: "nginx"
    annotations: {}
    hosts:
      - host: minio.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: []
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
    fsGroupChangePolicy: "OnRootMismatch"
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: false
  # Auto-create buckets on startup
  # Note: MinIO subchart has built-in bucket provisioning via 'buckets' config
  # The initJob below is an optional alternative for custom configurations
  buckets:
    - name: semantic-explorer-files
      policy: none
      purge: false
    - name: quickwit
      policy: none
      purge: false
  # Optional: Custom init job for bucket creation (alternative to subchart's built-in)
  # Enable this if you need custom bucket policies or the subchart provisioning fails
  initJob:
    enabled: true  # Create required buckets on startup

# Storage Configuration (S3-compatible)
# Configure external S3-compatible storage or use MinIO
storage:
  s3:
    # Use external S3 or S3-compatible storage (MinIO, AWS S3, etc.)
    endpoint: ""  # e.g., "http://minio:9000", "https://s3.amazonaws.com"
    region: "us-east-1"
    bucketName: "semantic-explorer-files"  # S3 bucket name for all collections and files
    # Force path-style addressing (bucket in URL path instead of subdomain)
    # Set to true for MinIO, Ceph, and other S3-compatible storage
    # Set to false for AWS S3 (uses virtual-host style: bucket.s3.region.amazonaws.com)
    forcePathStyle: false
    accessKeyId: ""
    secretAccessKey: ""
    existingSecret: ""
    accessKeyIdKey: "aws-access-key-id"
    secretAccessKeyKey: "aws-secret-access-key"
    # File Size Limits (in bytes)
    # Prevents memory exhaustion and DoS attacks
    maxDownloadSizeBytes: 104857600   # 100MB default - max file size for API downloads
    maxUploadSizeBytes: 1073741824    # 1GB default - max file size for uploads

# Encryption Configuration
# IMPORTANT: Configure master key for production deployments
encryption:
  # Master encryption key for API keys at rest (AES-256-GCM)
  # MUST be a 64-character hex string (256-bit key)
  # Generate with: openssl rand -hex 32
  # Example: "a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0c1d2e3f4a5b6c7d8e9f0a"
  # Set to empty string to disable (development only, will log warning)
  masterKey: ""  # Leave empty for development, set for production

# Observability Configuration (Optional Infrastructure)
observability:
  # OpenTelemetry Collector
  otelCollector:
    enabled: true
    image:
      registry: ""  # Defaults to global.imageRegistry
      repository: otel/opentelemetry-collector-contrib
      tag: "latest"
      pullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 2000m
        memory: 2Gi
    config:
      extensions:
        health_check:
          endpoint: 0.0.0.0:13133
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
              max_recv_msg_size_mib: 100
            http:
              endpoint: 0.0.0.0:4318
      processors:
        batch:
          timeout: 10s
          send_batch_size: 128
          send_batch_max_size: 256
        memory_limiter:
          check_interval: 1s
          limit_mib: 1024
          spike_limit_mib: 256
      exporters:
        prometheus:
          endpoint: "0.0.0.0:8889"
        debug:
          verbosity: detailed
        # Quickwit exporter for traces and logs (when enabled)
        otlp/quickwit:
          endpoint: "{{ include \"semantic-explorer.fullname\" . }}-quickwit:7281"
          tls:
            insecure: true
          retry_on_failure:
            enabled: true
            initial_interval: 5s
            max_interval: 30s
            max_elapsed_time: 300s
          sending_queue:
            enabled: true
            num_consumers: 10
            queue_size: 5000
      service:
        extensions: [health_check]
        pipelines:
          traces:
            receivers: [otlp]
            processors: [memory_limiter, batch]
            exporters: [otlp/quickwit, debug]
          metrics:
            receivers: [otlp]
            processors: [memory_limiter, batch]
            exporters: [prometheus, debug]
          logs:
            receivers: [otlp]
            processors: [memory_limiter, batch]
            exporters: [otlp/quickwit, debug]
    service:
      type: ClusterIP
      ports:
        otlpGrpc: 4317
        otlpHttp: 4318
        metrics: 8889
    podSecurityContext:
      runAsNonRoot: true
      runAsUser: 10001
      fsGroup: 10001
      fsGroupChangePolicy: "OnRootMismatch"
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: true
  
  # Prometheus configuration
  # See: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus
  prometheus:
    enabled: true  # Deploy Prometheus for metrics storage
    # Use existing Prometheus instance instead
    external:
      enabled: false
      url: ""  # e.g., "http://prometheus.monitoring:9090"

  # Quickwit (Log aggregation and tracing) - our own template, not a subchart
  quickwit:
    enabled: true  # Enable Quickwit for logs and traces
    # Use external Quickwit instance
    external:
      enabled: false
      url: ""  # e.g., "http://quickwit.example.com:7280"
    # Internal Quickwit deployment
    image:
      registry: ""  # Defaults to global.imageRegistry
      repository: quickwit/quickwit
      tag: "latest"
      pullPolicy: IfNotPresent
    replicaCount: 2  # 2 replicas for HA with shared S3 storage
    config:
      enableOtlpEndpoint: true
      logLevel: "info"
    # S3 storage configuration (used when minio.enabled=true)
    storage:
      bucketName: "quickwit"  # Dedicated bucket for Quickwit metastore and indexes
    resources:
      requests:
        cpu: 500m
        memory: 1Gi
      limits:
        cpu: 2000m
        memory: 4Gi
    persistence:
      enabled: true
      storageClass: ""  # Defaults to global.storageClass
      accessMode: ReadWriteOnce
      size: 50Gi
    service:
      type: ClusterIP
      rest:
        port: 7280
      grpc:
        port: 7281
    ingress:
      enabled: false
      className: "nginx"
      annotations: {}
      hosts:
        - host: quickwit.example.com
          paths:
            - path: /
              pathType: Prefix
      tls: []
    podSecurityContext:
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 1000
      fsGroupChangePolicy: "OnRootMismatch"
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: false
    metrics:
      enabled: true
      serviceMonitor:
        enabled: false
        interval: 30s
    nodeSelector: {}
    tolerations: []

# Prometheus subchart configuration (prometheus-community/prometheus)
prometheus:
  enabled: true  # Deploy Prometheus for metrics storage
  # RBAC configuration - disable subchart RBAC, we use our own namespace-scoped Role
  # See templates/prometheus-rbac.yaml for the namespaced Role/RoleBinding
  rbac:
    create: false  # Don't create ClusterRole/ClusterRoleBinding from subchart
  # Disable service account creation from subchart - we create our own
  serviceAccounts:
    server:
      create: true
      name: ""  # Uses fullname from subchart
    pushgateway:
      create: false
    alertmanager:
      create: false
  # Disable unused components
  alertmanager:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  # Server configuration
  server:
    retention: "15d"
    extraArgs:
      web.enable-remote-write-receiver: ""
    persistentVolume:
      enabled: true
      size: 50Gi
      storageClass: ""
    resources:
      requests:
        cpu: 500m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
      runAsGroup: 65534
      fsGroup: 65534
    containerSecurityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
  # Scrape configs using Kubernetes service discovery
  serverFiles:
    prometheus.yml:
      scrape_configs:
        # Prometheus self-scrape (internal metrics, no namespace needed)
        - job_name: prometheus
          static_configs:
            - targets:
                - localhost:9090
        # OTEL Collector metrics (exports app metrics in Prometheus format)
        - job_name: otel-collector
          scrape_interval: 15s
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                own_namespace: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: .*-otel-collector
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              regex: metrics
              action: keep
            # Add namespace label from kubernetes metadata
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
        # API service metrics - uses service name as job label
        - job_name: semantic-explorer-api
          scrape_interval: 15s
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                own_namespace: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: .*-api
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              regex: http
              action: keep
            # Add namespace label from kubernetes metadata
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
          metrics_path: /metrics
        # Worker Collections metrics
        - job_name: worker-collections
          scrape_interval: 15s
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                own_namespace: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: .*-worker-collections
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              regex: metrics
              action: keep
            # Add namespace label from kubernetes metadata
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
          metrics_path: /metrics
        # Worker Datasets metrics
        - job_name: worker-datasets
          scrape_interval: 15s
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                own_namespace: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: .*-worker-datasets
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              regex: metrics
              action: keep
            # Add namespace label from kubernetes metadata
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
          metrics_path: /metrics
        # Worker Visualizations (Python) metrics
        - job_name: worker-visualizations-py
          scrape_interval: 15s
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                own_namespace: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: .*-worker-visualizations-py
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              regex: metrics
              action: keep
            # Add namespace label from kubernetes metadata
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
          metrics_path: /metrics
        # Embedding Inference API metrics (GPU)
        - job_name: embedding-inference-api
          scrape_interval: 15s
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                own_namespace: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: .*-embedding-inference-api
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              regex: http
              action: keep
            # Add namespace label from kubernetes metadata
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
          metrics_path: /metrics
        # LLM Inference API metrics (GPU)
        - job_name: llm-inference-api
          scrape_interval: 15s
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                own_namespace: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: .*-llm-inference-api
              action: keep
            - source_labels: [__meta_kubernetes_endpoint_port_name]
              regex: http
              action: keep
            # Add namespace label from kubernetes metadata
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
          metrics_path: /metrics
  service:
    type: ClusterIP

# Grafana subchart configuration (grafana/grafana)
# See: https://github.com/grafana/helm-charts/tree/main/charts/grafana
grafana:
  enabled: true  # Deploy Grafana for metrics visualization
  # Use existing Grafana instance
  external:
    enabled: false
    url: ""  # e.g., "http://grafana.monitoring:3000"
  # Grafana subchart configuration
  image:
    tag: "11.6.1"
  adminUser: admin
  adminPassword: ""  # Generate random if empty
  
  # Custom Semantic Explorer datasource configuration
  # These settings are used by templates/grafana-datasources-configmap.yaml
  # Note: This is separate from grafana.datasources which is reserved by the Grafana subchart
  customDatasources:
    # PostgreSQL datasource for audit events dashboard
    # Automatically enabled when postgresql.enabled=true, or configure manually for external DB
    postgresAudit:
      enabled: false  # Set to true when using external PostgreSQL
      # External PostgreSQL connection settings (used when postgresql.enabled=false)
      host: ""  # e.g., "postgres.database.svc.cluster.local"
      port: 5432
      database: "semantic_explorer"
      user: ""  # PostgreSQL username
      password: ""  # PostgreSQL password
      # Connection pool settings
      sslmode: "disable"  # disable, require, verify-ca, verify-full
      maxOpenConns: 5  # Maximum open connections
      maxIdleConns: 2  # Maximum idle connections
      connMaxLifetime: 14400  # Connection max lifetime in seconds
      postgresVersion: 1500  # 1500 = PostgreSQL 15, 1400 = PostgreSQL 14, etc.
      timescaledb: false  # Enable TimescaleDB features if applicable
    
    # Prometheus datasource configuration
    prometheus:
      # Use external Prometheus instead of the bundled one
      external:
        enabled: false
        url: ""  # e.g., "http://prometheus.monitoring:9090"
    
    # Quickwit datasource configuration for logs and traces
    quickwit:
      # Log index name
      logsIndex: "otel-logs-v0_7"
      # Traces index name  
      tracesIndex: "otel-traces-v0_7"
  
  # Dashboard toggles to enable/disable specific dashboards
  # Note: Uses customDashboards to avoid conflict with Grafana subchart's dashboards key
  customDashboards:
    services:
      enabled: true  # Service health and HTTP metrics
    infrastructure:
      enabled: true  # PostgreSQL, Qdrant, NATS, MinIO, Quickwit, OTEL
    audit:
      enabled: true  # Audit events dashboard (requires postgres-audit datasource)
    logsTraces:
      enabled: true  # Logs and traces exploration (requires Quickwit)
    gpu:
      enabled: true  # GPU & Inference monitoring (requires embedding-inference-api or llm-inference-api)
  
  # RBAC configuration - use namespaced permissions only
  rbac:
    namespaced: true
  # Disable init container - not needed without persistence
  initChownData:
    enabled: false
  # Sidecar for auto-discovering dashboards from ConfigMaps
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"
      folder: /tmp/dashboards
      # Use null to search only the release namespace (namespace-scoped RBAC only)
      # Setting to ALL requires ClusterRole for cross-namespace access
      searchNamespace: null
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        runAsGroup: 10001
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault
    datasources:
      enabled: true
      label: grafana_datasource
      labelValue: "1"
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        runAsGroup: 10001
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
        seccompProfile:
          type: RuntimeDefault
  # Datasources are provided via ConfigMap with grafana_datasource label
  # See templates/grafana-datasources-configmap.yaml
  # Install Quickwit datasource plugin
  plugins:
    - quickwit-quickwit-datasource
  # Dashboards are auto-discovered by the sidecar from ConfigMaps with grafana_dashboard label
  # No need to configure dashboards here - they're loaded dynamically
  resources:
    requests:
      cpu: 250m
      memory: 256Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  persistence:
    enabled: true
    storageClassName: ""  # Defaults to global.storageClass
    accessModes:
      - ReadWriteOnce
    size: 5Gi
  service:
    type: ClusterIP
    port: 3000
  ingress:
    enabled: false
    ingressClassName: "nginx"
    annotations: {}
    hosts:
      - grafana.example.com
    tls: []
  securityContext:
    runAsNonRoot: true
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: false