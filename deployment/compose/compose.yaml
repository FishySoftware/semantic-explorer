
networks:
  semantic-explorer-net:
    driver: bridge

services:

  semantic-explorer:
    image: jofish89/semantic-explorer:latest
    depends_on:
      - minio-1
      - minio-2
      - minio-3
      - minio-4
      - minio-init
      - qdrant
      - dex
      - postgres
      - nats-1
      - nats-2
      - nats-3
      - otel-collector
    restart: unless-stopped
    environment:
      - HOSTNAME=0.0.0.0
      - PORT=8080
      - PUBLIC_URL=http://localhost:8080
      - RUST_LOG=warn,actix_web_prom=error,semantic_explorer=info
      - LOG_FORMAT=json
      - AWS_ACCESS_KEY_ID=test_access_key
      - AWS_SECRET_ACCESS_KEY=test_secret_key
      - AWS_REGION=us-east-1
      - AWS_ENDPOINT_URL=http://minio-1:9000
      - S3_BUCKET_NAME=semantic-explorer-local
      - S3_FORCE_PATH_STYLE=true
      - S3_MAX_DOWNLOAD_SIZE_BYTES=104857600
      - S3_MAX_UPLOAD_SIZE_BYTES=1073741824
      - QDRANT_URL=http://qdrant:6334
      - QDRANT_TIMEOUT_SECS=30
      - QDRANT_CONNECT_TIMEOUT_SECS=10
      - DATABASE_URL=postgresql://user:password@postgres:5432/explorer
      - DB_MIN_CONNECTIONS=2
      - DB_MAX_CONNECTIONS=20
      - DB_ACQUIRE_TIMEOUT_SECS=30
      - DB_IDLE_TIMEOUT_SECS=300
      - DB_MAX_LIFETIME_SECS=1800
      - OIDC_ISSUER_URL=http://dex:5556/dex
      - OIDC_CLIENT_ID=semantic-explorer
      - OIDC_CLIENT_SECRET=ZXhhbXBsZS1hcHAtc2VjcmV0
      - NATS_URL=nats://nats-1:4222
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - SERVICE_NAME=semantic-explorer
      - SHUTDOWN_TIMEOUT_SECS=30
      # Embedding and LLM inference API URLs
      - EMBEDDING_INFERENCE_API_URL=http://embedding-inference-api:8090
      - EMBEDDING_INFERENCE_API_TIMEOUT_SECS=120
      - LLM_INFERENCE_API_URL=http://llm-inference-api:8091
      - LLM_INFERENCE_API_TIMEOUT_SECS=120
      # API Key Encryption - IMPORTANT: Generate a unique key for production!
      # Generate with: openssl rand -hex 32
      - ENCRYPTION_MASTER_KEY=${ENCRYPTION_MASTER_KEY:-e9d7c7c2864110d9511423ec979416298843e39a2538d19eb1d94947c61b41c4}
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/health/live"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - semantic-explorer-net

  worker-collections:
    image: jofish89/worker-collections:latest
    depends_on:
      - nats-1
      - nats-2
      - nats-3
      - minio-1
      - minio-init
      - otel-collector
    restart: unless-stopped
    environment:
      - RUST_LOG=warn,actix_web_prom=error,worker_collections=info
      - LOG_FORMAT=json
      - NATS_URL=nats://nats-1:4222
      - AWS_ACCESS_KEY_ID=test_access_key
      - AWS_SECRET_ACCESS_KEY=test_secret_key
      - AWS_REGION=us-east-1
      - AWS_ENDPOINT_URL=http://minio-1:9000
      - S3_BUCKET_NAME=semantic-explorer-local
      - S3_FORCE_PATH_STYLE=true
      - S3_MAX_DOWNLOAD_SIZE_BYTES=104857600
      - S3_MAX_UPLOAD_SIZE_BYTES=1073741824
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - SERVICE_NAME=worker-collections
      - ENCRYPTION_MASTER_KEY=${ENCRYPTION_MASTER_KEY:-e9d7c7c2864110d9511423ec979416298843e39a2538d19eb1d94947c61b41c4}
    networks:
      - semantic-explorer-net

  worker-datasets:
    image: jofish89/worker-datasets:latest
    depends_on:
      - nats-1
      - nats-2
      - nats-3
      - minio-1
      - minio-init
      - qdrant
      - otel-collector
    restart: unless-stopped
    environment:
      - RUST_LOG=warn,actix_web_prom=error,worker_datasets=info
      - LOG_FORMAT=json
      - NATS_URL=nats://nats-1:4222
      - AWS_ACCESS_KEY_ID=test_access_key
      - AWS_SECRET_ACCESS_KEY=test_secret_key
      - AWS_REGION=us-east-1
      - AWS_ENDPOINT_URL=http://minio-1:9000
      - S3_BUCKET_NAME=semantic-explorer-local
      - S3_FORCE_PATH_STYLE=true
      - S3_MAX_DOWNLOAD_SIZE_BYTES=104857600
      - S3_MAX_UPLOAD_SIZE_BYTES=1073741824
      - QDRANT_URL=http://qdrant:6334
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - SERVICE_NAME=worker-datasets
      - ENCRYPTION_MASTER_KEY=${ENCRYPTION_MASTER_KEY:-e9d7c7c2864110d9511423ec979416298843e39a2538d19eb1d94947c61b41c4}
    networks:
      - semantic-explorer-net

  worker-visualizations-py:
    image: jofish89/worker-visualizations-py:latest
    depends_on:
      - nats-1
      - nats-2
      - nats-3
      - qdrant
      - minio-1
      - minio-init
      - otel-collector
    restart: unless-stopped
    environment:
      - NATS_URL=nats://nats-1:4222
      - NATS_STREAM_RETRY_ATTEMPTS=30
      - NATS_STREAM_RETRY_DELAY=2.0
      - NATS_BATCH_SIZE=1
      - NATS_FETCH_TIMEOUT=5.0
      - QDRANT_URL=http://qdrant:6334
      - AWS_ENDPOINT_URL=http://minio-1:9000
      - AWS_ACCESS_KEY_ID=test_access_key
      - AWS_SECRET_ACCESS_KEY=test_secret_key
      - AWS_REGION=us-east-1
      - S3_BUCKET_NAME=semantic-explorer-local
      - S3_FORCE_PATH_STYLE=true
      - S3_MAX_DOWNLOAD_SIZE_BYTES=104857600
      - S3_MAX_UPLOAD_SIZE_BYTES=1073741824
      - LOG_LEVEL=INFO
      - WORKER_ID=py-worker-prod-1
      - PROCESSING_TIMEOUT_SECS=3600
      - MAX_CONCURRENT_JOBS=3
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - PROMETHEUS_METRICS_PORT=9093
      - SERVICE_NAME=worker-visualizations-py
      - ENCRYPTION_MASTER_KEY=${ENCRYPTION_MASTER_KEY:-e9d7c7c2864110d9511423ec979416298843e39a2538d19eb1d94947c61b41c4}
    networks:
      - semantic-explorer-net

  # Embedding inference service for embeddings and reranking (CUDA GPU accelerated)
  embedding-inference-api:
    image: jofish89/embedding-inference-api:cuda
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - RUST_LOG=warn,embedding_inference_api=info
      - LOG_FORMAT=json
      - INFERENCE_HOSTNAME=0.0.0.0
      - INFERENCE_PORT=8090
      # GPU Configuration: Set CUDA_VISIBLE_DEVICES to control which GPU(s) to use
      # Examples: CUDA_VISIBLE_DEVICES=0 (first GPU), CUDA_VISIBLE_DEVICES=0,1 (multiple GPUs)
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      # Preload embedding and reranker models at startup
      - INFERENCE_PRELOAD_MODELS=BAAI/bge-small-en-v1.5,BAAI/bge-reranker-base
      - INFERENCE_DEFAULT_EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
      - INFERENCE_DEFAULT_RERANKER_MODEL=BAAI/bge-reranker-base
      - INFERENCE_MAX_BATCH_SIZE=256
      # HuggingFace cache for models (mount a volume for persistence)
      - HF_HOME=/models
      # For airgapped deployments: pre-download models and mount at HF_HOME
      # For proxy deployments: set HF_ENDPOINT to your Artifactory proxy
      # - HF_ENDPOINT=https://artifactory.company.com/huggingface
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - SERVICE_NAME=embedding-inference-api
      - ENCRYPTION_MASTER_KEY=${ENCRYPTION_MASTER_KEY:-e9d7c7c2864110d9511423ec979416298843e39a2538d19eb1d94947c61b41c4}
    volumes:
      - inference_models:/models:rw
      - tensorrt_cache:/tensorrt-cache:rw
    ports:
      - "8090:8090"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8090/health/live"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 120s  # Allow time for model downloads on first run
    networks:
      - semantic-explorer-net

  # LLM inference service for text generation (CUDA GPU accelerated)
  llm-inference-api:
    image: jofish89/llm-inference-api:cuda
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - RUST_LOG=warn,llm_inference_api=info
      - LOG_FORMAT=json
      - LLM_INFERENCE_HOSTNAME=0.0.0.0
      - LLM_INFERENCE_PORT=8091
      - CORS_ALLOWED_ORIGINS=*
      # GPU Configuration: Set CUDA_VISIBLE_DEVICES to control which GPU(s) to use
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
      # Model configuration
      - LLM_ALLOWED_MODELS=mistralai/Mistral-7B-Instruct-v0.2
      - LLM_DEFAULT_MODEL=mistralai/Mistral-7B-Instruct-v0.2
      - LLM_MAX_CONCURRENT_REQUESTS=10
      - LLM_DEFAULT_TEMPERATURE=0.7
      - LLM_DEFAULT_TOP_P=0.9
      - LLM_DEFAULT_MAX_TOKENS=512
      - LLM_MAX_TOKENS_LIMIT=4096
      # HuggingFace cache for models (mount a volume for persistence)
      - HF_HOME=/models
      # For airgapped deployments: pre-download models and mount at HF_HOME
      # For proxy deployments: set HF_ENDPOINT to your Artifactory proxy
      # - HF_ENDPOINT=https://artifactory.company.com/huggingface
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - SERVICE_NAME=llm-inference-api
      - ENCRYPTION_MASTER_KEY=${ENCRYPTION_MASTER_KEY:-e9d7c7c2864110d9511423ec979416298843e39a2538d19eb1d94947c61b41c4}
    volumes:
      - llm_models:/models:rw
    ports:
      - "8091:8091"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8091/health/live"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 120s  # Allow time for model downloads on first run
    networks:
      - semantic-explorer-net

  # Console: http://localhost:9001
  minio-1:
    image: minio/minio:latest
    hostname: minio-1
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: test_access_key
      MINIO_ROOT_PASSWORD: test_secret_key
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_SCANNER_SPEED: default
    command: server http://minio-{1...4}:9000/data --address ":9000" --console-address ":9001"
    volumes:
      - minio_data_1:/data:rw
    ports:
      - "9000:9000"
      - "9001:9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - semantic-explorer-net

  minio-2:
    image: minio/minio:latest
    hostname: minio-2
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: test_access_key
      MINIO_ROOT_PASSWORD: test_secret_key
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_SCANNER_SPEED: default
    command: server http://minio-{1...4}:9000/data --address ":9000" --console-address ":9001"
    volumes:
      - minio_data_2:/data:rw
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - semantic-explorer-net

  minio-3:
    image: minio/minio:latest
    hostname: minio-3
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: test_access_key
      MINIO_ROOT_PASSWORD: test_secret_key
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_SCANNER_SPEED: default
    command: server http://minio-{1...4}:9000/data --address ":9000" --console-address ":9001"
    volumes:
      - minio_data_3:/data:rw
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - semantic-explorer-net

  minio-4:
    image: minio/minio:latest
    hostname: minio-4
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: test_access_key
      MINIO_ROOT_PASSWORD: test_secret_key
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_SCANNER_SPEED: default
    command: server http://minio-{1...4}:9000/data --address ":9000" --console-address ":9001"
    volumes:
      - minio_data_4:/data:rw
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - semantic-explorer-net

  # Initialize MinIO: create bucket
  minio-init:
    image: minio/mc:latest
    depends_on:
      minio-1:
        condition: service_healthy
      minio-2:
        condition: service_healthy
      minio-3:
        condition: service_healthy
      minio-4:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c '
      echo "Waiting for cluster to stabilize...";
      sleep 5;

      echo "Configuring MinIO client...";
      mc alias set myminio http://minio-1:9000 test_access_key test_secret_key;

      echo "Creating bucket: semantic-explorer-local";
      mc mb myminio/semantic-explorer-local --ignore-existing;

      echo "Setting bucket policy to allow read/write";
      mc anonymous set download myminio/semantic-explorer-local;

      echo "MinIO cluster status:";
      mc admin info myminio;

      echo "MinIO initialization complete!";
      '
    networks:
      - semantic-explorer-net
    restart: "no"

  qdrant:
    image: qdrant/qdrant:gpu-nvidia-latest
    hostname: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage:rw
    environment:
      - QDRANT__GPU__INDEXING=1
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - semantic-explorer-net

  postgres:
    image: postgres:16.3-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: explorer
    volumes:
      - postgres_data:/var/lib/postgresql/data:rw
    ports:
      - "5432:5432"
    networks:
      - semantic-explorer-net

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: postgresql://user:password@postgres:5432/explorer?sslmode=disable
    ports:
      - "9187:9187"
    depends_on:
      - postgres
    networks:
      - semantic-explorer-net

  dex:
    image: dexidp/dex:latest
    command: dex serve /etc/dex/config.yaml
    environment:
      GITHUB_CLIENT_ID: ${GITHUB_CLIENT_ID}
      GITHUB_CLIENT_SECRET: ${GITHUB_CLIENT_SECRET}
    restart: unless-stopped
    volumes:
      - ./dex.yaml:/etc/dex/config.yaml:ro
      - dex_data:/var/dex:rw
    ports:
      - "5556:5556"
      - "5557:5557"
      - "5558:5558"
    networks:
      - semantic-explorer-net

  nats-1:
    image: nats:2.10-alpine
    hostname: nats-1
    ports:
      - "4222:4222"
      - "8222:8222"
      - "6222:6222"
    command:
      - "-js"
      - "-sd"
      - "/data"
      - "-m"
      - "8222"
      - "-cluster"
      - "nats://0.0.0.0:6222"
      - "-cluster_name"
      - "semantic-explorer-nats"
      - "-routes"
      - "nats://nats-1:6222,nats://nats-2:6223,nats://nats-3:6224"
      - "-name"
      - "nats-1"
    volumes:
      - nats_data_1:/data:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8222/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - semantic-explorer-net

  nats-2:
    image: nats:2.10-alpine
    hostname: nats-2
    ports:
      - "4223:4223"
      - "8223:8223"
      - "6223:6223"
    command:
      - "-js"
      - "-sd"
      - "/data"
      - "-p"
      - "4223"
      - "-m"
      - "8223"
      - "-cluster"
      - "nats://0.0.0.0:6223"
      - "-cluster_name"
      - "semantic-explorer-nats"
      - "-routes"
      - "nats://nats-1:6222,nats://nats-2:6223,nats://nats-3:6224"
      - "-name"
      - "nats-2"
    volumes:
      - nats_data_2:/data:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8223/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - semantic-explorer-net

  nats-3:
    image: nats:2.10-alpine
    hostname: nats-3
    ports:
      - "4224:4224"
      - "8224:8224"
      - "6224:6224"
    command:
      - "-js"
      - "-sd"
      - "/data"
      - "-p"
      - "4224"
      - "-m"
      - "8224"
      - "-cluster"
      - "nats://0.0.0.0:6224"
      - "-cluster_name"
      - "semantic-explorer-nats"
      - "-routes"
      - "nats://nats-1:6222,nats://nats-2:6223,nats://nats-3:6224"
      - "-name"
      - "nats-3"
    volumes:
      - nats_data_3:/data:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8224/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - semantic-explorer-net

  quickwit:
    image: quickwit/quickwit:latest
    ports:
      - "7280:7280"
      - "7281:7281"
    volumes:
      - quickwit_data:/quickwit/qwdata:rw
      - ./quickwit-config.yaml:/quickwit/node.yaml:ro
    environment:
      - QW_ENABLE_OTLP_ENDPOINT=true
      - QW_ENABLE_JAEGER_ENDPOINT=true
      - RUST_LOG=info
    command: ["run"]
    restart: unless-stopped
    networks:
      - semantic-explorer-net

  prometheus:
    image: prom/prometheus:latest
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-remote-write-receiver'
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus:rw
    restart: unless-stopped
    networks:
      - semantic-explorer-net

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"
      - "4318:4318"
      - "8888:8888"
      - "8889:8889"
      - "13133:13133"
    depends_on:
      - quickwit
      - prometheus
    restart: unless-stopped
    networks:
      - semantic-explorer-net

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    volumes:
      - grafana_data:/var/lib/grafana:rw
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - quickwit
    restart: unless-stopped
    networks:
      - semantic-explorer-net

volumes:
  qdrant_storage:
  minio_data_1:
  minio_data_2:
  minio_data_3:
  minio_data_4:
  postgres_data:
  dex_data:
  nats_data_1:
  nats_data_2:
  nats_data_3:
  quickwit_data:
  prometheus_data:
  grafana_data:
  inference_models:
  tensorrt_cache:
  llm_models:
