
services:

  semantic-explorer:
    image: jofish89/semantic-explorer:latest
    depends_on:
      - rustfs
      - qdrant
      - dex
      - postgres
      - nats-1
      - nats-2
      - nats-3
      - redis-1
      - redis-2
      - redis-3
      - redis-cluster-init
      - otel-collector
    restart: unless-stopped
    environment:
      - HOSTNAME=localhost
      - PORT=8080
      - RUST_LOG=warn,actix_web_prom=error,semantic_explorer=debug
      - LOG_FORMAT=json
      - AWS_ACCESS_KEY_ID=test_access_key
      - AWS_SECRET_ACCESS_KEY=test_secret_key
      - AWS_REGION=us-east-1
      - AWS_ENDPOINT_URL=http://127.0.0.1:9000
      - S3_BUCKET_NAME=semantic-explorer-local
      - QDRANT_URL=http://localhost:6334
      - QDRANT_CONNECT_TIMEOUT_SECS=10
      - DATABASE_URL=postgresql://user:password@localhost:5432/explorer
      - DB_MIN_CONNECTIONS=2
      - DB_MAX_CONNECTIONS=20
      - DB_ACQUIRE_TIMEOUT_SECS=30
      - DB_IDLE_TIMEOUT_SECS=300
      - DB_MAX_LIFETIME_SECS=1800
      - OIDC_ISSUER_URL=http://localhost:5556/dex
      - OIDC_CLIENT_ID=semantic-explorer
      - OIDC_CLIENT_SECRET=ZXhhbXBsZS1hcHAtc2VjcmV0
      - NATS_URL=nats://localhost:4222
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
      - SERVICE_NAME=semantic-explorer
      - SHUTDOWN_TIMEOUT_SECS=30
      # API Key Encryption - IMPORTANT: Generate a unique key for production!
      # Generate with: openssl rand -hex 32
      - ENCRYPTION_MASTER_KEY=${ENCRYPTION_MASTER_KEY:-e9d7c7c2864110d9511423ec979416298843e39a2538d19eb1d94947c61b41c4}
      # Redis Cluster Configuration
      - REDIS_CLUSTER_NODES=redis://localhost:7000,redis://localhost:7001,redis://localhost:7002
      - REDIS_POOL_SIZE=10
      - REDIS_CONNECT_TIMEOUT_SECS=5
      # Rate Limiting Configuration (high initial limits for gradual rollout)
      - RATE_LIMIT_ENABLED=true
      - RATE_LIMIT_DEFAULT_REQUESTS_PER_MINUTE=1000
      - RATE_LIMIT_SEARCH_REQUESTS_PER_MINUTE=600
      - RATE_LIMIT_CHAT_REQUESTS_PER_MINUTE=200
      - RATE_LIMIT_TRANSFORM_REQUESTS_PER_MINUTE=100
      - RATE_LIMIT_TEST_REQUESTS_PER_MINUTE=100
      # CORS_ALLOWED_ORIGINS=https://example.com,https://app.example.com
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/health/live"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    network_mode: host

  worker-collections:
    image: jofish89/worker-collections:latest
    depends_on:
      - nats-1
      - nats-2
      - nats-3
      - rustfs
      - otel-collector
    restart: unless-stopped
    environment:
      - RUST_LOG=warn,actix_web_prom=error,worker_collections=debug
      - LOG_FORMAT=json
      - NATS_URL=nats://localhost:4222
      - AWS_ACCESS_KEY_ID=test_access_key
      - AWS_SECRET_ACCESS_KEY=test_secret_key
      - AWS_REGION=us-east-1
      - AWS_ENDPOINT_URL=http://127.0.0.1:9000
      - S3_BUCKET_NAME=semantic-explorer-local
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
      - SERVICE_NAME=worker-collections
    network_mode: host

  worker-datasets:
    image: jofish89/worker-datasets:latest
    depends_on:
      - nats-1
      - nats-2
      - nats-3
      - rustfs
      - qdrant
      - otel-collector
    restart: unless-stopped
    environment:
      - RUST_LOG=warn,actix_web_prom=error,worker_datasets=debug
      - LOG_FORMAT=json
      - NATS_URL=nats://localhost:4222
      - AWS_ACCESS_KEY_ID=test_access_key
      - AWS_SECRET_ACCESS_KEY=test_secret_key
      - AWS_REGION=us-east-1
      - AWS_ENDPOINT_URL=http://127.0.0.1:9000
      - S3_BUCKET_NAME=semantic-explorer-local
      - QDRANT_URL=http://localhost:6334
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
      - SERVICE_NAME=worker-datasets
    network_mode: host

  worker-visualizations-py:
    image: jofish89/worker-visualizations-py:latest
    depends_on:
      - nats-1
      - nats-2
      - nats-3
      - qdrant
      - rustfs
      - otel-collector
    restart: unless-stopped
    environment:
      - NATS_URL=nats://localhost:4222
      - QDRANT_URL=http://localhost:6334
      - S3_ENDPOINT=http://localhost:9000
      - AWS_ACCESS_KEY_ID=test_access_key
      - AWS_SECRET_ACCESS_KEY=test_secret_key
      - AWS_REGION=us-east-1
      - S3_BUCKET_NAME=semantic-explorer-local
      - LOG_LEVEL=INFO
      - WORKER_ID=py-worker-prod-1
      - PROCESSING_TIMEOUT_SECS=3600
      - MAX_CONCURRENT_JOBS=3
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
      - SERVICE_NAME=worker-visualizations
    network_mode: host

  rustfs:
    image: rustfs/rustfs:latest
    volumes:
      - rustfs_data:/data:rw
      - rustfs_logs:/logs:rw
    environment:
      - RUSTFS_ACCESS_KEY=test_access_key
      - RUSTFS_SECRET_KEY=test_secret_key
      - AWS_REGION=us-east-1
    restart: unless-stopped
    ports:
      - "9000:9000"
      - "9001:9001"
    network_mode: host

  qdrant:  
    image: qdrant/qdrant:gpu-nvidia-latest
    hostname: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage:rw
    environment:
      - QDRANT__GPU__INDEXING=1
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    network_mode: host
    
  postgres:
    image: postgres:16.3-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: explorer
    volumes:
      - postgres_data:/var/lib/postgresql/data:rw
    ports:
      - "5432:5432"
    network_mode: host

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: postgresql://user:password@localhost:5432/explorer?sslmode=disable
    ports:
      - "9187:9187"
    depends_on:
      - postgres
    network_mode: host

  # Redis Cluster for rate limiting and caching (3 nodes)
  redis-1:
    image: redis:7-alpine
    hostname: redis-1
    restart: unless-stopped
    ports:
      - "7000:7000"
      - "17000:17000"
    command: >
      redis-server
      --port 7000
      --cluster-enabled yes
      --cluster-config-file nodes-7000.conf
      --cluster-node-timeout 5000
      --appendonly yes
      --bind 0.0.0.0
      --protected-mode no
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data_1:/data:rw
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "7000", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    network_mode: host

  redis-2:
    image: redis:7-alpine
    hostname: redis-2
    restart: unless-stopped
    ports:
      - "7001:7001"
      - "17001:17001"
    command: >
      redis-server
      --port 7001
      --cluster-enabled yes
      --cluster-config-file nodes-7001.conf
      --cluster-node-timeout 5000
      --appendonly yes
      --bind 0.0.0.0
      --protected-mode no
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data_2:/data:rw
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "7001", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    network_mode: host

  redis-3:
    image: redis:7-alpine
    hostname: redis-3
    restart: unless-stopped
    ports:
      - "7002:7002"
      - "17002:17002"
    command: >
      redis-server
      --port 7002
      --cluster-enabled yes
      --cluster-config-file nodes-7002.conf
      --cluster-node-timeout 5000
      --appendonly yes
      --bind 0.0.0.0
      --protected-mode no
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data_3:/data:rw
    healthcheck:
      test: ["CMD", "redis-cli", "-p", "7002", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    network_mode: host

  # Redis cluster initialization (runs once to create cluster)
  redis-cluster-init:
    image: redis:7-alpine
    depends_on:
      redis-1:
        condition: service_healthy
      redis-2:
        condition: service_healthy
      redis-3:
        condition: service_healthy
    command: >
      sh -c '
      echo "Waiting for Redis nodes to be ready...";
      sleep 5;
      redis-cli --cluster create
      127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002
      --cluster-replicas 0
      --cluster-yes || true;
      echo "Redis cluster initialized";
      '
    network_mode: host
    restart: "no"

  dex:
    image: dexidp/dex:latest
    command: dex serve /etc/dex/config.yaml
    environment:
      GITHUB_CLIENT_ID: ${GITHUB_CLIENT_ID}
      GITHUB_CLIENT_SECRET: ${GITHUB_CLIENT_SECRET}
    restart: unless-stopped
    volumes:
      - ./dex.yaml:/etc/dex/config.yaml:ro
      - dex_data:/var/dex:rw
    ports:
      - "5556:5556"
      - "5557:5557"
      - "5558:5558"
    network_mode: host

  nats-1:
    image: nats:2.10-alpine
    hostname: nats-1
    ports:
      - "4222:4222"
      - "8222:8222"
      - "6222:6222"
    command:
      - "-js"
      - "-sd"
      - "/data"
      - "-m"
      - "8222"
      - "-cluster"
      - "nats://0.0.0.0:6222"
      - "-cluster_name"
      - "semantic-explorer-nats"
      - "-routes"
      - "nats://localhost:6222,nats://localhost:6223,nats://localhost:6224"
      - "-name"
      - "nats-1"
    volumes:
      - nats_data_1:/data:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8222/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    network_mode: host

  nats-2:
    image: nats:2.10-alpine
    hostname: nats-2
    ports:
      - "4223:4223"
      - "8223:8223"
      - "6223:6223"
    command:
      - "-js"
      - "-sd"
      - "/data"
      - "-p"
      - "4223"
      - "-m"
      - "8223"
      - "-cluster"
      - "nats://0.0.0.0:6223"
      - "-cluster_name"
      - "semantic-explorer-nats"
      - "-routes"
      - "nats://localhost:6222,nats://localhost:6223,nats://localhost:6224"
      - "-name"
      - "nats-2"
    volumes:
      - nats_data_2:/data:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8223/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    network_mode: host

  nats-3:
    image: nats:2.10-alpine
    hostname: nats-3
    ports:
      - "4224:4224"
      - "8224:8224"
      - "6224:6224"
    command:
      - "-js"
      - "-sd"
      - "/data"
      - "-p"
      - "4224"
      - "-m"
      - "8224"
      - "-cluster"
      - "nats://0.0.0.0:6224"
      - "-cluster_name"
      - "semantic-explorer-nats"
      - "-routes"
      - "nats://localhost:6222,nats://localhost:6223,nats://localhost:6224"
      - "-name"
      - "nats-3"
    volumes:
      - nats_data_3:/data:rw
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8224/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    network_mode: host


  quickwit:
    image: quickwit/quickwit:latest
    ports:
      - "7280:7280"
      - "7281:7281"
    volumes:
      - quickwit_data:/quickwit/qwdata:rw
      - ./quickwit-config.yaml:/quickwit/node.yaml:ro
    environment:
      - QW_ENABLE_OTLP_ENDPOINT=true
      - QW_ENABLE_JAEGER_ENDPOINT=true
      - RUST_LOG=info
    command: ["run"]
    restart: unless-stopped
    network_mode: host

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"
      - "4318:4318"
      - "8888:8888"
      - "8889:8889"
      - "13133:13133"
    depends_on:
      - quickwit
      - prometheus
    restart: unless-stopped
    network_mode: host

  prometheus:
    image: prom/prometheus:latest
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus:rw
    restart: unless-stopped
    network_mode: host

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    volumes:
      - grafana_data:/var/lib/grafana:rw
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
      - quickwit
    restart: unless-stopped
    network_mode: host
      
volumes:
  qdrant_storage:
  rustfs_data:
  rustfs_logs:
  postgres_data:
  dex_data:
  redis_data_1:
  redis_data_2:
  redis_data_3:
  nats_data_1:
  nats_data_2:
  nats_data_3:
  quickwit_data:
  prometheus_data:
  grafana_data: